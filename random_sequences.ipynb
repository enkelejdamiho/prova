{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import string\n",
    "import random\n",
    "import Levenshtein\n",
    "import networkx as nx\n",
    "from scipy.sparse import coo_matrix\n",
    "from sys import argv\n",
    "from math import factorial\n",
    "random.seed(1)\n",
    "start_time = time.clock()\n",
    "\n",
    "minimum_aa = 4\n",
    "maximum_aa = 20\n",
    "min_seqs = 0\n",
    "max_seqs = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations will be 1249975000\n"
     ]
    }
   ],
   "source": [
    "print 'Number of iterations will be %d'%(factorial(max_seqs)/2/(factorial(max_seqs-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os, sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2adba86fa890>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executor.instances',  100)\n",
    "conf.set('spark.executor.cores', 4)\n",
    "conf.set('spark.executor.memory', '9g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='yarn-client', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "string_list = []\n",
    "\n",
    "for i in range(min_seqs, max_seqs):\n",
    "    random_integer = random.randrange(minimum_aa, maximum_aa)\n",
    "    s = ''.join(random.choice('ACDEFGHIKLMNPQRSTVWY') for _ in range(random_integer))\n",
    "    string_list.append(s)\n",
    "\n",
    "d = {}\n",
    "d_i = {}\n",
    "for i, s in enumerate(string_list):\n",
    "    d[i] = s\n",
    "    d_i[s] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Npart = 400\n",
    "n_per = max_seqs/Npart/2\n",
    "\n",
    "def balance_partitions(x): \n",
    "    part = 0\n",
    "    if x < max_seqs/2:\n",
    "        part = x/n_per\n",
    "    else:\n",
    "        part = (max_seqs-x)/n_per\n",
    "    return part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx_rdd = sc.parallelize(xrange(max_seqs), Npart).map(lambda x: (x,None)).partitionBy(Npart, balance_partitions).keys()\n",
    "string_rdd = sc.parallelize(string_list, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings_b = sc.broadcast(string_list)\n",
    "d_b = sc.broadcast(d)\n",
    "d_i_b = sc.broadcast(d_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix_elements_idx(iterator):\n",
    "    string_map = d_b.value\n",
    "    \n",
    "    for my_idx in iterator: \n",
    "        for idx in range(my_idx):\n",
    "            ld = Levenshtein.distance(string_map[my_idx], string_map[idx])\n",
    "            if ld < 2 and ld > 0: \n",
    "                yield (my_idx, idx, float(ld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_rdd.cache().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6200000"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_rdd.mapPartitions(get_len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2718\n",
      "CPU times: user 33 ms, sys: 15 ms, total: 48 ms\n",
      "Wall time: 8.26 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "mat = idx_rdd.mapPartitions(generate_matrix_elements_idx)\n",
    "print mat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_matrix_elements(string_list):\n",
    "    levenshtein_list = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    for a, b in itertools.combinations(string_list, 2):\n",
    "        levenshtein_distance = Levenshtein.distance(a, b)\n",
    "        if levenshtein_distance < 2:\n",
    "            levenshtein_list.append(1)\n",
    "            rows.append(d_i[a])\n",
    "            cols.append(d_i[b])\n",
    "    return levenshtein_list, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nx.write_gml(g, \"g_500K.gml\")\n",
    "\n",
    "\n",
    "text_file = open(\"output_500K.txt\", \"w\")\n",
    "text_file.write(' '.join(('Number of generated sequences:', str(len(string_list)), 'sequences'))) \n",
    "text_file.write(' '.join(('\\nTime calculating the Levenshtein distance:', str(dist_time), 'seconds')))\n",
    "text_file.write(' '.join(('\\nTime calculating the graph:', str(graph_time), 'seconds')))\n",
    "text_file.write(' '.join(('\\nTotal pipeline time:', str(total_time), 'seconds')))\n",
    "text_file.write('\\nSuccessful finish')\n",
    "text_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
