{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "import string\n",
    "import random\n",
    "import Levenshtein\n",
    "import networkx as nx\n",
    "from scipy.sparse import coo_matrix\n",
    "import rpy2.robjects as robjects\n",
    "from sys import argv\n",
    "from math import factorial\n",
    "random.seed(1)\n",
    "start_time = time.clock()\n",
    "\n",
    "minimum_aa = 4\n",
    "maximum_aa = 20\n",
    "min_seqs = 0\n",
    "max_seqs = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the original function -- not used anymore\n",
    "def get_matrix_elements(string_list):\n",
    "    levenshtein_list = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    for a, b in itertools.combinations(string_list, 2):\n",
    "        levenshtein_distance = Levenshtein.distance(a, b)\n",
    "        if levenshtein_distance < 2:\n",
    "            levenshtein_list.append(1)\n",
    "            rows.append(d_i[a])\n",
    "            cols.append(d_i[b])\n",
    "    return levenshtein_list, rows, cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations will be 124999750000\n"
     ]
    }
   ],
   "source": [
    "print 'Number of iterations will be %d'%(max_seqs*(max_seqs-1)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import os, sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['SPARK_DRIVER_MEMORY'] = '1500m'\n",
    "\n",
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define spark runtime parameters\n",
    "number_of_executors = 100\n",
    "cores_per_executor = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x2b6013840cd0>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set('spark.executor.instances',  number_of_executors)\n",
    "conf.set('spark.executor.cores', cores_per_executor)\n",
    "conf.set('spark.executor.memory', '9g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(master='yarn-client', conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate the string list and lookup dictionaries (including reverse lookup)\n",
    "\n",
    "string_list = []\n",
    "\n",
    "for i in range(min_seqs, max_seqs):\n",
    "    random_integer = random.randrange(minimum_aa, maximum_aa)\n",
    "    s = ''.join(random.choice('ACDEFGHIKLMNPQRSTVWY') for _ in range(random_integer))\n",
    "    string_list.append(s)\n",
    "\n",
    "string_map = {}\n",
    "string_map_inverse = {}\n",
    "for i, s in enumerate(string_list):\n",
    "    string_map[i] = s\n",
    "    string_map_inverse[s] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def balance_partitions(x, N_partitions, N_elements): \n",
    "    \"\"\"Balances out the number of elements across the partitions\"\"\"\n",
    "    n_per_part = N_elements/N_partitions/2\n",
    "    \n",
    "    part = 0\n",
    "    if x > N_elements/2:\n",
    "        x = N_elements-x\n",
    "    part = x/n_per_part\n",
    "    return part if part < N_partitions else part-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "number_of_partitions = number_of_executors*cores_per_executor*10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create an RDD of indices and balance partitioning\n",
    "idx_rdd = sc.parallelize(xrange(max_seqs))\\\n",
    "            .map(lambda x: (x,None))\\\n",
    "            .partitionBy(number_of_partitions, lambda x: balance_partitions(x, number_of_partitions, max_seqs)).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# broadcast the lookup dictionaries\n",
    "string_map_b = sc.broadcast(string_map)\n",
    "string_map_inverse_b = sc.broadcast(string_map_inverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_matrix_elements_idx(iterator):\n",
    "    \"\"\"\n",
    "    Generates non-zero matrix elements by calculating the Levenshtein\n",
    "    distance for each string in the partition against all the strings in the \n",
    "    lookup dictionary. It only searches the strings with 0 < idx < my_idx, \n",
    "    i.e. only filling in the upper triangle of the matrix.\n",
    "    \"\"\"    \n",
    "    \n",
    "    lookup = string_map_b.value\n",
    "    \n",
    "    for my_idx in iterator: \n",
    "        for idx in range(my_idx):\n",
    "            ld = Levenshtein.distance(lookup[my_idx], lookup[idx])\n",
    "            if ld < 2 and ld > 0: \n",
    "                yield (my_idx, idx, float(ld))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as  np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.48 s, sys: 221 ms, total: 1.7 s\n",
      "Wall time: 8min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mat = idx_rdd.mapPartitions(generate_matrix_elements_idx)\n",
    "mat_data = np.array(mat.collect())\n",
    "distance_matrix = sparse.csr_matrix((mat_data[:,2], (mat_data[:,0], mat_data[:,1])), (max_seqs, max_seqs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make the graph\n",
    "import networkx\n",
    "\n",
    "g = networkx.Graph(distance_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
